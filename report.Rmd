---
title: "News-popularity"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
library(tidyverse)
library(readr)
library(purrr)
library(tidyr)
library(ggplot2)
library(caret)
library(data.table)
library(modelsummary)
library(kableExtra)

library(gbm)
# library(xgboost)
library(pROC)
library(ROCR)
library(skimr)
library(h2o)
library(bit64)
# library(rpart)
# library(rpart.plot)



my_seed <- 20210410
```


# Intro

This is a report for the binary classification competition for the course Data Science 2: Machine Learning Tools, at CEU in the Spring semester of 2020/21.

In this competition, I try to predict which articles are shared the most on social media. The data comes from the website mashable.com from the beginning of 2015. The dataset used in the competition is from the UCI repository.



# EDA


Train and test datasets come from Data section of the competition.

This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict if the article is among the most popular ones based on sharing in social networks (coded by the variable "is_popular").

```{r, message=FALSE, echo=FALSE}
train_raw <- read_csv("data/train.csv")
test_raw <- read_csv("data/test.csv")
```

Train dataset contains 27752 observations and 60 variables. 

Test dataset contains 11892 observations and 59 variables. 


```{r, warning=FALSE, message=FALSE, echo=FALSE}
# datasummary_skim(train_raw) 
# skimr::skim(train_raw)
```




Training dataset contains 60 columns, all numeric. None of them contains missing values. 

Even though we have a lot of columns, some of them are actually dummy variables. For example, 7 variables that starts with week_ (r.g. weekday_is_monday) shows whether the article was published on that day (Monday). is_weekend is also dummy variable that contains only values of 1 and 0 that shows whether the article was published on the weekend. 6 other dummy variables that starts with data_ (e.g. data_channel_is_lifestyle) shows the channel of the news (data channel 'Lifestyle'). 


```{r}
table(train_raw$is_popular)
```

Popular news are less than not popular news: there are 5466 popular news and 22286 not popular news. 


```{r}
table(train_raw$is_weekend)
```
3638 news were published at the weekend while 24114 news were published on weekdays. 


```{r}
ggplot(data=train_raw, aes(train_raw$is_popular)) +
  geom_boxplot()


```


```{r}
table(train_raw$is_weekend)
```

Few articles published in weekend. 



```{r}

train_raw %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```


# Data cleaning

The following variables are dummy variables: 

- 6 variables that start with data_

- 7 variables that start with week_

- is_weekend variable

I convert them from numeric into factor variables. 

Also, our target variable is *is_popular* variable. I also convert it into factor variable and give proper labels (yes, no) instead of 1 and 0.  

Now we have 15 factor and 45 numeric variables. 

```{r}
cols <- c("is_weekend", 
          colnames(train_raw[ , grepl( "weekday" , names( train_raw ) ) ]),
          colnames(train_raw[ , grepl( "data" , names( train_raw ) ) ]))
train_raw[,cols] <- lapply(train_raw[, cols], factor)

train_raw <- 
  train_raw %>% mutate(
    is_popular = factor(is_popular, levels = list(1,0), labels = c('yes', 'no'))) %>% as.data.frame()


test_raw[,cols] <- lapply(test_raw[, cols], factor)

```



# Model


Let's start building models using the train data. 


## 1. Linear model 

Linear Probability Model produces values less than 0 or more than 1. But Logit model regularizes this process and gives results of 1 and 0 only. Therefore, I decided to use Logit for Linear Model. 

I used 5 fold cross-validation and set class probability for training control.  

```{r, warning=FALSE, echo=FALSE}

train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE
)
```

### Parameter tuning

Since we have a lot of columns, we need regularization technique. I used Logit LASSO and tried to identify best lambda and alpha values. 

```{r, warning=FALSE, message=FALSE}

# Logit lasso -----------------------------------------------------------

lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

set.seed(my_seed)
logit_lasso_model <- train(
    is_popular ~ .,
    data = train_raw,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )

tuned_logit_lasso_model <- logit_lasso_model$finalModel
# best_lambda <- logit_lasso_model$bestTune$lambda
logit_lasso_model$bestTune
```

Best lambda was 0.0004641589 and best alpha was 1. 

```{r}
# logit_lasso_model
cv_roc_folds <- logit_lasso_model$resample[,c("Resample", "ROC")]

logit_lasso_model$results[order(logit_lasso_model$results$ROC, decreasing = TRUE),][1,"ROC"]
```
ROC of Logit LASSO model is 0.6937. 

### Evaluation

Now it's time to make predictions on the test data. We saved predicted probabilities as 'score' variable. Then we make a dataframe with 'score' and 'article_id' columns. This dataframe contains our predictions. 

```{r}

test_raw[, 'score'] <- predict(logit_lasso_model, newdata = test_raw, type = "prob") %>%
  as.data.frame() %>% 
  select(yes)

to_submit <- test_raw[, c("article_id", "score")]

write.csv(to_submit, "submit_news_2.csv", row.names=FALSE)
```

I turned dataframe into CSV and submitted to Kaggle. The AUC of Logit LASSO model in Kaggle was 0.68890. 




```{r}
cv_fold <- logit_lasso_model$pred %>% 
  filter(Resample == "Fold1")
roc_obj <- roc(cv_fold$obs, cv_fold$yes)
roc_obj$auc
```
Area under the curve of LOGIT LASSO is 0.6758. 



## 2. Random Forest 

In most cases Random Forest captures non-linear relationships better than Linear Models.

### Parameter tuning 

Random Forest has several tuning parameters. For number of variables, I took the square root of the total number of variables, which would be 7 (approx. square root of 60) so I tried 7, 8, and 9. For the split rule, I took gini. I also took combination of 5 and 10 for the minimum node size.



```{r rf_tune, echo = FALSE, message=FALSE, warning=FALSE}
# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary,
                              savePredictions = TRUE,
                              verboseIter = FALSE)

set.seed(my_seed)
# set tuning
tune_grid <- expand.grid(
  .mtry = c(7,8,9),
  .splitrule = c("gini"),
  .min.node.size = c(5, 10)
)

# rf_model_1 <- train(
#   is_popular ~ ., 
#   data = train_raw,
#   method = "ranger",
#   trControl = train_control,
#   tuneGrid = tune_grid,
#   importance = "impurity"
# )

# head(rf_model_1$results[order(rf_model_1$results$ROC, decreasing = TRUE),], 1)
```

Tuning shows 7 as a number of variable and 10 as minimum node size gives best result. ROC is 0.7089616. 

### Evaluation

We predict probability in test set using Random Forest model this time. Dataframe that contains our predictions is turned into CSV and submitted to Kaggle. 

```{r}

# summary(rf_model_1)

# test_raw[, 'score'] <- predict(rf_model_1, newdata = test_raw, type = "prob") %>%
#   as.data.frame() %>% 
#   select(yes)
# 
# to_submit <- test_raw[, c("article_id", "score")]
# 
# write.csv(to_submit, "submit_news_1.csv", row.names=FALSE)

```

Kaggle gives 0.71139 AUC for Random Forest prediction. So far this is the best result I could achieve.


## 3. Gradient Boosting 

Like random forests, gradient boosting is a set of decision trees. The two main differences are:

How trees are built: random forests builds each tree independently while gradient boosting builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners.

Combining results: random forests combine results at the end of the process (by averaging or "majority rules") while gradient boosting combines results along the way.
If we carefully tune parameters, gradient boosting can result in better performance than random forests.


### Parameter tuning 

These are tuning parameters for the Gradient Boosting Machine: complexity of tree, number of trees, learning rate (how quickly the algorithm adapts), minimum samples. 
I took combination of 1, 5, and 10 for the complexity of the tree. For the number of trees, I experimented starting from 200 to 500 (stepping by 50). I decided to play around 10 values from 0.01 to 0.3 for the learning rate (shrinkage). 1 and 5 were investigated for the the minimum number of training set samples in a node to commence splitting. 



```{r gbm_tune, echo = FALSE, message=FALSE, warning=FALSE}

train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  savePredictions = TRUE
)

gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), 
                         n.trees = (4:10)*50, 
                         shrinkage = seq(0.01,0.3,length=10), 
                         n.minobsinnode = c(1, 5))


set.seed(my_seed)

# gbm_model <- train(
#     is_popular ~ .,
#     data = train_raw,
#     method = "gbm",
#     trControl = train_control,
#     verbose = FALSE,
#     tuneGrid = gbm_grid)


# gbm_model$results[order(gbm_model$results$Accuracy, decreasing = TRUE),][1,]

```

The final values used for the model were n.trees = 300, interaction.depth = 10, shrinkage = 0.01 and n.minobsinnode = 1.
 
The Accuracy for the GBM is 0.804987.


### Evaluation

```{r}
# test_raw[, 'score'] <- predict(gbm_model, newdata = test_raw, type = "prob") %>%
#   as.data.frame() %>% 
#   select(yes)
# 
# to_submit <- test_raw[, c("article_id", "score")]
# 
# write.csv(to_submit, "submit_news_3.csv", row.names=FALSE)
```

When the model prediction is submitted to Kaggle my result was 0.70643. This result is worse than random forest. This is probably I had to use different tuning technique because in practice GBM shows better result than Random Forest. 


## 4. Neural Network 

For our binary classification, now we experiment with Neural networks.  

Neural networks are multi-layer networks of neurons that we use to classify things or make predictions. In NN we have inputs, outputs, and hidden layers of neurons.

We have size parameter = number of nodes in the hidden layer. 

Also decay = regularization for weight to avoid over-fitting.

For size we try 3, 5, 7, 10, 15. For decay we try 0.1, 0.5, 1, 1.5, 2, 2.5, 5. 

```{r, warning=FALSE}
tune_grid_nnet <- expand.grid(
  size = c(3, 5, 7, 10, 15),
  decay = c(0.1, 0.5, 1, 1.5, 2, 2.5, 5)
)

set.seed(my_seed)

# nnet_model <- train(
#   is_popular ~ .,
#   method = "nnet",
#   data = train_raw,
#   trControl = train_control,
#   tuneGrid = tune_grid_nnet,
#   preProcess = c("center", "scale", "pca"),
#   metric = "ROC",
#   trace = FALSE
# )

```

Our best tune indicated size 7 and decay 2. 

### Evaluation

```{r}
# test_raw[, 'score'] <- predict(nnet_model, newdata = test_raw, type = "prob") %>%
#   as.data.frame() %>% 
#   select(yes)
# 
# to_submit <- test_raw[, c("article_id", "score")]
# 
# write.csv(to_submit, "submit_news_4.csv", row.names=FALSE)
```

When we submitted our predictions to Kaggle based on Neural Network, we obtained 0.69465 as result. 

In practice, neural network should achieve better performance than any earlier used models. The reason for this bad perfromance may be caused by the activation function. The further steps may include to experiment with sigmoid activation function as we are doing binary classification. 





# For extra points, build a stacked model and submit the prediction to Kaggle, explain how it works, and evaluate its results in the Rmd document.

I used h2o to build a stacked model using all 4 models trained earlier. 

```{r, warning=FALSE, results='hide'}
# Initialize H2O
h2o.no_progress() 
h2o.init(max_mem_size = "8g")
# Create H2O datasets
data_train <- as.h2o(train_raw)
data_test <- as.h2o(test_raw)
# Set the target variable
y <- "is_popular"
X <- setdiff(names(data_train), y)
```

```{r, warning=FALSE}
# four_models <- list(logit_lasso_model, rf_model_1, gbm_model, nnet_model)

```



### Stacked ensemble model from the base learners

```{r stack, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# ensemble_model <- h2o.stackedEnsemble(
#   X, y,
#   training_frame = data_train,
#   base_models = four_models,
#   seed = my_seed,
#   keep_levelone_frame = TRUE
# )

```


It took me more than 10 hours to train. I had to stop execution as the deadline was approaching.

# Conclusion

I tried 4 models for binary classification: Logit LASSO, Random Forest, GBM and Neural Network. So far, Random Forest result helped me to rank 13th in the Kaggle competition.

If I could dedicate more days, applied smarter data pre-processing (Data Cleaning and Feature Engineering), worked with better computation power and implemented more suitable tuning techniques, I think I could achieve better rank.

This competition was very interesting to participate and gave me useful experience. It was fun and I learned that smarter and more suitable techniques provides better results. 





